{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import gym\n",
    "\n",
    "import imitation_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes  = 100\n",
    "num_steps     = 5\n",
    "batch_size    = 2\n",
    "n_iterations  = 10\n",
    "\n",
    "input_size    = 5\n",
    "action_size   = 4\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_policy_loss(p, a, r):\n",
    "    \"\"\"\n",
    "    p - array (n_episodes x n_steps x state_size)\n",
    "    a - array (n_episodes x n_steps x num_actions) - binary array\n",
    "    r - array (n_episodes x n_steps)\n",
    "    \"\"\"\n",
    "    n_episodes = a.shape[0]\n",
    "    n_steps    = a.shape[1]\n",
    "    n_actions  = a.shape[2]\n",
    "    \n",
    "    log_pr     = tf.zeros(shape=[n_steps, n_actions])\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        r_cum = tf.cumsum(r[i], reverse=True)\n",
    "        r_cum = tf.expand_dims(r_cum, axis=1)\n",
    "        \n",
    "        log_pr += -tf.math.log(p)*a[i]*r_cum\n",
    "        \n",
    "    return tf.reduce_sum(log_pr)/n_episodes\n",
    "\n",
    "def gradient(model, s, a, r):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss = discrete_policy_loss(model(s),a,r)\n",
    "    return t.gradient(loss, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=[input_size]),\n",
    "    keras.layers.Dense(action_size, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0').env.__class__(\n",
    "    map_name='4x4', is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = imitation_learning.agent.DiscreteActionAgent(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim   = imitation_learning.simulator.Simulator(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer   = tf.optimizers.SGD(learning_rate=0.0001, momentum=0.9)\n",
    "\n",
    "global_step = tf.Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 204.18272399902344\n",
      "1: 202.64610290527344\n",
      "2: 200.51953125\n",
      "3: 197.93185424804688\n",
      "4: 195.01260375976562\n",
      "5: 191.8876953125\n",
      "6: 188.6752166748047\n",
      "7: 185.4822998046875\n",
      "8: 182.40228271484375\n",
      "9: 179.51268005371094\n"
     ]
    }
   ],
   "source": [
    "inds = np.arange(num_episodes)\n",
    "for it in range(n_iterations):\n",
    "    \n",
    "    I = np.random.choice(inds, size=batch_size)\n",
    "    \n",
    "    sb = np.ones((num_episodes, num_steps, input_size), dtype=np.float32)[I]\n",
    "    ab = np.ones((num_episodes, num_steps, action_size), dtype=np.float32)[I]\n",
    "    rb = np.ones((num_episodes, num_steps), dtype=np.float32)[I]\n",
    "    \n",
    "    g = gradient(model,sb,ab,rb)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(g,model.trainable_variables), global_step)\n",
    "    \n",
    "    l = discrete_policy_loss(model(sb),ab,rb)\n",
    "    \n",
    "    print(\"{}: {}\".format(it,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09393696 0.24695668 0.33182007 0.3272863 ]\n",
      " [0.09393696 0.24695668 0.33182007 0.3272863 ]\n",
      " [0.09393696 0.24695668 0.33182007 0.3272863 ]\n",
      " [0.09393696 0.24695668 0.33182007 0.3272863 ]\n",
      " [0.09393696 0.24695668 0.33182007 0.3272863 ]]\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(sb[0])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "(4,)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a = agent.act(sb[0,0])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
